\documentclass[12pt,a4paper]{article}
\usepackage[left=2.5cm,top=2cm,right=2.5cm,bottom=2cm,nofoot]{geometry}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[UKenglish]{babel}
\usepackage[UKenglish]{isodate}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{systeme}
\usepackage{booktabs}
\usepackage{lmodern}
\usepackage{hyperref}
\usepackage{icomma}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{textcomp}
\usepackage[parfill]{parskip}
\usepackage[title,titletoc]{appendix}
\usepackage{multicol}
\usepackage{float}
\usepackage{gensymb}
\usepackage{url}
\usepackage{subcaption}
\usepackage{esvect}
\usepackage{mathtools}
\usepackage{bm}
\usepackage{cancel}
\usepackage{comment}
\usepackage{makecell}
\usepackage{mathtools}
\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}
\usepackage[numbered,framed]{matlab-prettifier}
\definecolor{mygreen}{RGB}{28,172,0} % color values Red, Green, Blue
\definecolor{mylilas}{RGB}{170,55,241}
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}
\lstdefinelanguage{Julia}%
  {morekeywords={abstract,break,case,catch,const,continue,do,else,elseif,%
      end,export,false,for,function,immutable,import,importall,if,in,%
      macro,module,otherwise,quote,return,switch,true,try,type,typealias,%
      using,while},%
   sensitive=true,%
   alsoother={\$},%
   morecomment=[l]\#,%
   morecomment=[n]{\#=}{=\#},%
   morestring=[s]{"}{"},%
   morestring=[m]{'}{'},%
}[keywords,comments,strings]%

\lstset{%
    language         = Julia,
    basicstyle       = \ttfamily,
    keywordstyle     = \bfseries\color{blue},
    stringstyle      = \color{magenta},
    commentstyle     = \color{ForestGreen},
    showstringspaces = false,
}
\lstset{style=mystyle}
\pagestyle{myheadings}
\pagenumbering{empty}

\title{Project EEN020 Computer Vision}
\author{Erik Norlin}
\date{\today}

\begin{document}
\cleanlookdateon
\maketitle
\begin{center}
\setlength{\tabcolsep}{12pt}
\begin{tabular}{cc}
\end{tabular}

\end{center}
\vspace{2.0cm}

\pagenumbering{arabic}

\section*{Algorithm}

The algorithm consists of four parts; rotation averaging to estimate the absolute rotations of the cameras, translation registration to estimate the translations of the cameras, camera refinement to adjust the cameras, and triangulation of the final 3D-reconstruction.

\subsection*{Rotation averaging}
All absolute rotations are obtained by chaining the relative rotations of each adjacent camera pair as $R_j=R_{i,j}R_i$. The relative rotations are extracted from robustly estimated essential matrices and normalized by taking the singular value decomposition of $R_{i,j}$ and setting $R_{i,j}=UV^T$.

Essential matrices for each adjacent camera pair are robustly estimated using image correspondences obtained from SIFT-points in a RANSAC loop. \texttt{estimate\_E\_robust} from assignment 4 is extended by extracting essential matrices from an estimated homography, checking the validity of every estimated essential matrix, and dynamically changing the number of RANSAC iterations $T_E$ and $T_H$ by computing

\begin{equation}\label{eq:T}
T = \ceil*{\frac{\ln(1-\alpha)}{\ln (1-\epsilon^s)}}
\end{equation}

In each iteration both an essential matrix and a homogropahy are estimated for minimal samples using an 8-point DLT method and a 4-point DLT method respectively. From the homography, two other essential matrices can be extracted. The reason for extracting essential matrices from a homography is that as the scene becomes more planar the risk of degenerate solutions obtained by the 8-point method increases, whereas a homography becomes better at mapping the image points of two cameras facing the scene. Inliers for an essential matrix are measured with point-to-line distance (point to epipolar line), and inliers for a homography are measured with point-to-point distance (point to projected point). The pixel-threshold is three times larger for computing inliers for the homography than the essential matrices. A valid essential matrix has rank 2 and is therefore always tested for this. If a new best essential matrix is estimated but is not rank 2 then it is discarded. $T_E$ and $T_H$ are adjusted every time a better valid essential matrix is found. 

When RANSAC has terminated, four cameras are extracted from the best essential matrix \textit{outside} the RANSAC loop to speed up the process. The cheirality test is perfomed on these cameras, and the camera that yields the most inliers in front of both cameras is selected as the correct solution. For the vast majority of the time this works wonderful. However in rare cases, triangulating 3D-points using the correct extracted camera from the returned essential matrix results in non-sensical "sprays" of point clouds.

\subsection*{Translation registration}
An initial 3D-reconstruction is triangulated with relative cameras and image correspondences (SIFT-matches) from a camera pair with a sufficiently large baseline. Outliers are removed as well as 10\% of the 3D-points being the furthest away from the center of gravity of the point cloud. The initial 3D-reconstruction is not rotated nor centered because the canonical camera in the common coordinate frame is set to be camera $P_i$ of the initial pair. Hence, rotation and centering of the 3D-points are not needed.

2D-3D correspondences are established for each camera by matching descriptors corresponding to the 3D-points and the descriptors for the image points obtained from SIFT. 

Camera translations are robustly estimated in a RANSAC loop one by one for a minimal sample of 2 by solving $\textbf{T}_i$ in eq. \ref{eq:1}

\begin{equation}\label{eq:1}
\begin{pmatrix}
[\textbf{x}_{ij}]_\times & [\textbf{x}_{ij}]_\times R_i
\end{pmatrix}
\begin{pmatrix}
\textbf{T}_i \\ \textbf{X}_j
\end{pmatrix}
= \textbf{0}
\end{equation}

3D points are projected onto the image plane using the absolute rotation and an estimate of the translation vector, and the 2D-3D inliers are measured using point-to-point distance between the projected points and the corresponding image points.

Initially, the translations were solved from a 2-point method obtained by simplifying \texttt{estimate\_P\_DLT} from assignment 2. Unfortunately, this method does not result in inliers for all cameras, only for some, even with extremely large values for the pixel-threshold. At times, some cameras only get one inlier, and these initial estimates turns out to never be good enough for the bundle adjustment to yield satisfactory final 3D-reconstructions. Instead, solving $\textbf{T}_i$ from eq. \ref{eq:1} for a minimal sample of two showed to result in more inliers and better 3D-reconstructions after bundle adjustment. Enough inliers for all translations are still not obtained by doing this, though. On top of this, when triangulating 3D-reconstructions using bad robust estimates of the translations, optimized or not, the SVD can sometime have a hard time converging resulting in throwing an exception. With all these issues at hand, non-robust translations are also estimated using all 2D-3D correspondences as a compromise to get translation vectors for all cameras.

\subsection*{Camera refinement}
The absolute rotations and non-robust translations are refined by minimizing the squared reprojection error using Levenberg-Marquardt (LM) where the rotations are parametrized as quaternions when being optimized. More specifically, LM is implemented using Scipy's least squares module where it optimizes with respect to all rotations (quaternion parameters) and translations simultaneously.


\subsection*{Final 3D-reconstruction}
Two final 3D-reconstructions are obtained by accumulating triangulated 3D-points using cameras and image correspondences for each adjacent camera pair. The first showing the reconstruction of the un-refined cameras pairs, and the other showing the reconstruction of the refined cameras pairs.


\section*{Implementation and discussion}

There were two main issues with getting the implementation to work. 

Using a simplified version of \texttt{estimate\_P\_DLT} from assignment 2 was expected for the robust estimation of the translations. 

The latter resulted in more inliers and better estimates for more translations and became therefore the method of choice. However, this did still not give estimates for all translations. For every dataset, there were always some cameras that never had \textit{any} inliers. Recalling collecting the number of inliers for a computer exercise in assignment 3, using matched SIFT-points from VLFEAT yielded about 1000 times more inliers than using matched SIFT-points from OpenCV. For this reason, a python wrapper for VLFEAT (ubcmatch not included) called "cyvlfeat" was implemented. Using SIFT-points obtained from cyvlfeat did not result in estimates for all translations either. Matching descriptors in Matlab did not make it better either. Since the number of inliers obtained from translation registration are already relatively low due to noise in the rotations, not having a large enough amount of good SIFT-matches from the start could potentially be enough of an issue to not yield a sufficient amount of inliers for translation registration to work. Though, it still remains unclear why the translation registration did not work properly. 

Another issue that was never resolved was the LM algorithm. A modified version of the LM from assignment 4 was implemented where the rotation and the translations were optimized instead of the 3D-points. The rotations were first parametrized to axis-angle representation but doing this was not feasible because the rotations of the cameras were so small. The angle representation was practically zero resulting in singularity in the parametrization. Instead, the rotations were parametrized as quaternions which avoided this issue. The jacobian of the residual with respect to each parameter to optimize over was calculated as


$$
\frac{\partial\textbf{r}}{\partial \textbf{T}_i^1} =
\begin{bmatrix}
\frac{-1}{R_i^3\textbf{X}_j+\textbf{T}_i^3} \\
\textbf{0}
\end{bmatrix}
\text{, }
\frac{\partial\textbf{r}}{\partial \textbf{T}_i^2} =
\begin{bmatrix}
\textbf{0}
\\
\frac{-1}{R_i^3\textbf{X}_j+\textbf{T}_i^3} \\
\end{bmatrix}
\text{, }
\frac{\partial\textbf{r}}{\partial \textbf{T}_i^3} =
\begin{bmatrix}
\frac{R_i^1\textbf{X}_j+\textbf{T}_i^1}{(R_i^3\textbf{X}_j+\textbf{T}_i^3)^2} \\
\\
\frac{R_i^2\textbf{X}_j+\textbf{T}_i^2}{(R_i^3\textbf{X}_j+\textbf{T}_i^3)^2} 
\end{bmatrix}
$$

and


Running the optimization barely had an impact because the total reprojection error of the few 2D-3D inliers estimated previously was close to zero even before optimization, essentially leading to returning the same cameras initially. As a way of having a reference, Scipy's LM module for least squares was implemented. Refining the cameras one by one was first tested but resulted in poor 3D-reconstructions. Refining all cameras simultaneously yielded satisfactory results. The 3D-reconstructions from each adjacent camera pair aligned closely with each other with slight offset, which is reasonable since this is not a full scale bundle adjustment.

This version is less interpretable but does a very good job.

Triangulating 3D-points with some cameras resulted in SVD not converging. It was 


Document threshold change


Initial 3D essential, thr*3

\section*{Running the software}

The modules that needs to be installed to run this software are
\begin{itemize}
\item \texttt{argparse}
\item \texttt{matplotlib}
\item \texttt{numpy}
\item \texttt{opencv-python}
\item \texttt{scipy}
\item \texttt{tqdm}
\end{itemize}

The files of the actual implementation are
\begin{itemize} 
\item \texttt{computer\_vision.py}
\item \texttt{get\_dataset\_info.py}
\item \texttt{main.py}
\item \texttt{pipeline.py}
\end{itemize}
which need to be in the working directory together with a \texttt{data} folder containing numerated sub-folders with data sets.

Run the software with \texttt{main.py}. Two arguments can be passed in; \texttt{-dataset} (required) and \texttt{-T\_robust} (not required). \texttt{-dataset} takes an integer and searches for the specified data set in the \texttt{data} folder. \texttt{-T\_robust} is a flag and if passed in, the translation vectors will be estimated robustly, but the convergence of SVD can occasionally fail when triangulating the final 3D-reconstruction due to the poor estimates of the translations. If left out, all 2D-3D correspondences will be used to estimate the translation vectors. Leaving \texttt{-T\_robust} out yields significantly better 3D-reconstructions after bundle adjustment in this implementation. 


\section*{Reconstruction of the data sets with \texttt{T\_robust=False}}
Figures \ref{fig:d3b}-\ref{fig:d9a} show reconstructions of the data sets before and after LM optimization. The 3D-point clouds obtained from triangulation from all adjacent camera pairs have different colors to visually differentiate them. The reconstructions look horrendous before optimization, and afterwards we can see that the point clouds overlap each other with some offset showing that LM has improved the solutions. The final 3D-reconstructions of the data sets are not close to perfect, but considering that a full-scale bundle adjustment was not implemented, and that the translations of the cameras in the figures were not robustly estimated, lower quality of the reconstructions is expected.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{dataset_2_before_LM_2.png}
    \caption{Reconstruction of data set 3 before LM.}
    \label{fig:d3b}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{dataset_2_after_LM_2.png}
    \caption{Reconstruction of data set 3 after LM.}
    \label{fig:d3a}
\end{figure}


\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{dataset_3_before_LM_2.png}
    \caption{Reconstruction of data set 4 before LM.}
    \label{fig:d4b}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{dataset_3_after_LM_2.png}
    \caption{Reconstruction of data set 4 after LM.}
    \label{fig:d4a}
\end{figure}


\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{dataset_4_before_LM_2.png}
    \caption{Reconstruction of data set 5 before LM.}
    \label{fig:d5b}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{dataset_4_after_LM_2.png}
    \caption{Reconstruction of data set 5 after LM.}
    \label{fig:d5a}
\end{figure}


\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{dataset_5_before_LM_2.png}
    \caption{Reconstruction of data set 6 before LM.}
    \label{fig:d6b}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{dataset_5_after_LM_2.png}
    \caption{Reconstruction of data set 6 after LM.}
    \label{fig:d6a}
\end{figure}


\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{dataset_6_before_LM_2.png}
    \caption{Reconstruction of data set 7 before LM.}
    \label{fig:d7b}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{dataset_6_after_LM_2.png}
    \caption{Reconstruction of data set 7 after LM, view from above. This was the most difficult data set of all for this implementation. As one can see, the different reconstructions of the fasade are very poorly aligned.}
    \label{fig:d7a}
\end{figure}


\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{dataset_7_before_LM_2.png}
    \caption{Reconstruction of data set 8 before LM.}
    \label{fig:d8b}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{dataset_7_after_LM_2.png}
    \caption{Reconstruction of data set 8 after LM.}
    \label{fig:d8a}
\end{figure}


\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{dataset_8_before_LM_2.png}
    \caption{Reconstruction of data set 9 before LM.}
    \label{fig:d9b}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{dataset_8_after_LM_2.png}
    \caption{Reconstruction of data set 9 after LM.}
    \label{fig:d9a}
\end{figure}



\end{document}