{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import linalg as LA\n",
    "from scipy.io import loadmat\n",
    "import scipy\n",
    "from mpl_toolkits import mplot3d\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from matplotlib.pyplot import cm\n",
    "import matplotlib as mpl\n",
    "import cv2\n",
    "import computer_vision as cv\n",
    "from tqdm import trange\n",
    "import time\n",
    "from get_dataset_info import *\n",
    "from scipy.spatial.transform import Rotation\n",
    "# import cyvlfeat as vl\n",
    "\n",
    "# %load_ext snakeviz\n",
    "# %matplotlib inline\n",
    "%matplotlib qt\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "from matplotlib import rc\n",
    "rc('font', **{'family': 'serif', 'serif': ['Computer Modern']})\n",
    "rc('text', usetex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_3D_points(X):\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    ax = plt.axes(projection='3d')\n",
    "    ax.plot(X[0], X[1], X[2], '.', ms=1, color='magenta', label='X')\n",
    "    ax.set_xlabel('$x$')\n",
    "    ax.set_ylabel('$y$')\n",
    "    ax.set_zlabel('$z$')\n",
    "    # ax.set_box_aspect([1, 1, 1]) \n",
    "    ax.set_aspect('equal')\n",
    "    # ax.view_init(elev=-50, azim=-104, roll=20)\n",
    "    ax.legend(loc=\"lower right\")\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_E_validity(E):\n",
    "    rank = LA.matrix_rank(E)\n",
    "    valid = True if rank == 2 else False\n",
    "    return valid\n",
    "\n",
    "def compute_E_inliers(E, x1_norm, x2_norm, err_threshold):\n",
    "    \n",
    "    distance1_arr, distance2_arr = cv.compute_epipolar_errors(E, x1_norm, x2_norm)\n",
    "    inliers = ((distance1_arr**2 + distance2_arr**2) / 2) < err_threshold**2\n",
    "    n_inliers = np.sum(inliers)\n",
    "    epsilon_E = n_inliers / x1_norm.shape[1]\n",
    "\n",
    "    return epsilon_E, inliers\n",
    "\n",
    "def verbose_E_robust(t, T_E, T_H, epsilon_E, epsilon_H, inliers, method):\n",
    "    print('Iteration:', t, 'T_E:', T_E, 'T_H:', T_H, 'epsilon_E:', np.round(epsilon_E, 2), 'epsilon_H:', np.round(epsilon_H, 2), 'No. inliers:', np.sum(inliers), 'From:', method)\n",
    "\n",
    "def compute_valid_inliers(P1, P2, X, inliers):\n",
    "\n",
    "    x1_norm_valid = P1 @ X\n",
    "    x2_norm_valid = P2 @ X\n",
    "    valid_coords_P1 = x1_norm_valid[-1,:] > 0\n",
    "    valid_coords_P2 = x2_norm_valid[-1,:] > 0\n",
    "    valid_coords = valid_coords_P1 * valid_coords_P2\n",
    "    valid_inliers = inliers * valid_coords\n",
    "\n",
    "    return valid_inliers\n",
    "\n",
    "def estimate_E_robust(K, x1_norm, x2_norm, min_its, max_its, scale_its, alpha, err_threshold_px, essential_matrix=True, homography=True, verbose=False):\n",
    "    \n",
    "    err_threshold = err_threshold_px / K[0,0]\n",
    "    best_E = None\n",
    "    best_inliers = None\n",
    "    n_points = x1_norm.shape[1]\n",
    "    n_E_samples = 8\n",
    "    n_H_samples = 4\n",
    "    best_epsilon_E = 0\n",
    "    best_epsilon_H = 0\n",
    "    T_E = max_its\n",
    "    T_H = max_its\n",
    "\n",
    "    t = 0\n",
    "    while t < T_E and t < T_H:\n",
    "        t += 1\n",
    "\n",
    "        if essential_matrix:\n",
    "            rand_mask = np.random.choice(n_points, n_E_samples, replace=False)\n",
    "            E = cv.estimate_E_DLT(x1_norm[:,rand_mask], x2_norm[:,rand_mask], enforce=True, verbose=False)\n",
    "            E_valid = compute_E_validity(E)\n",
    "\n",
    "            if E_valid:\n",
    "                epsilon_E, inliers = compute_E_inliers(E, x1_norm, x2_norm, err_threshold)\n",
    "                    \n",
    "                if epsilon_E > best_epsilon_E:\n",
    "                    best_E = np.copy(E)\n",
    "                    best_inliers = np.copy(inliers)\n",
    "                    best_epsilon_E = epsilon_E\n",
    "                    T_E = cv.compute_ransac_iterations(alpha, best_epsilon_E, n_E_samples, min_its, max_its, scale_its)\n",
    "\n",
    "                    if verbose:\n",
    "                        verbose_E_robust(t, T_E, T_H, best_epsilon_E, best_epsilon_H, best_inliers, method='E 8-point alg.')\n",
    "        \n",
    "        if homography:\n",
    "            rand_mask = np.random.choice(n_points, n_H_samples, replace=False)\n",
    "            H = cv.estimate_H_DLT(x1_norm[:,rand_mask], x2_norm[:,rand_mask], verbose=False)\n",
    "            x2_norm_proj = cv.dehomogenize(H @ x1_norm)\n",
    "            distance_arr = cv.compute_point_point_distance(x2_norm_proj, x2_norm)\n",
    "            inliers = distance_arr < err_threshold\n",
    "            n_inliers = np.sum(inliers)\n",
    "            epsilon_H = n_inliers / n_points\n",
    "\n",
    "            if epsilon_H > best_epsilon_H:\n",
    "                \n",
    "                # num, Rs, Ts, Ns = cv2.decomposeHomographyMat(H, np.eye(3))\n",
    "                R1, T1, R2, T2 = cv.homography_to_RT(H, x1_norm, x2_norm)\n",
    "                E1 = cv.compute_E_from_R_and_T(R1, T1)\n",
    "                E2 = cv.compute_E_from_R_and_T(R2, T2)\n",
    "\n",
    "                E1_valid = compute_E_validity(E1)\n",
    "                E2_valid = compute_E_validity(E2)\n",
    "\n",
    "                if E1_valid:\n",
    "                    epsilon_E, inliers = compute_E_inliers(E1, x1_norm, x2_norm, err_threshold)\n",
    "                        \n",
    "                    if epsilon_E > best_epsilon_E:\n",
    "                        best_E = np.copy(E1)\n",
    "                        best_inliers = np.copy(inliers)\n",
    "                        best_epsilon_E = epsilon_E\n",
    "                        best_epsilon_H = epsilon_H\n",
    "                        T_E = cv.compute_ransac_iterations(alpha, best_epsilon_E, n_E_samples, min_its, max_its, scale_its)\n",
    "                        T_H = cv.compute_ransac_iterations(alpha, best_epsilon_H, n_H_samples, min_its, max_its, scale_its)\n",
    "\n",
    "                        if verbose:\n",
    "                            verbose_E_robust(t, T_E, T_H, best_epsilon_E, best_epsilon_H, best_inliers, method='H 4-point alg.')\n",
    "\n",
    "                if E2_valid:\n",
    "                    epsilon_E, inliers = compute_E_inliers(E2, x1_norm, x2_norm, err_threshold)\n",
    "                        \n",
    "                    if epsilon_E > best_epsilon_E:\n",
    "                        best_E = np.copy(E2)\n",
    "                        best_inliers = np.copy(inliers)\n",
    "                        best_epsilon_E = epsilon_E\n",
    "                        best_epsilon_H = epsilon_H\n",
    "                        T_E = cv.compute_ransac_iterations(alpha, best_epsilon_E, n_E_samples, min_its, max_its, scale_its)\n",
    "                        T_H = cv.compute_ransac_iterations(alpha, best_epsilon_H, n_H_samples, min_its, max_its, scale_its)\n",
    "                        \n",
    "                        if verbose:\n",
    "                            verbose_E_robust(t, T_E, T_H, best_epsilon_E, best_epsilon_H, best_inliers, method='H 4-point alg.')\n",
    "        \n",
    "    print('Bailout at iteration:', t)\n",
    "    return best_E, best_inliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_sift_points(img1, img2, marg, flann=False, verbose=False):\n",
    "    img1 = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)\n",
    "    img2 = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    sift = cv2.SIFT_create()\n",
    "    kp1, des1 = sift.detectAndCompute(img1, None)\n",
    "    kp2, des2 = sift.detectAndCompute(img2, None)\n",
    "\n",
    "    if flann:\n",
    "        FLANN_INDEX_KDTREE = 1\n",
    "        index_params = dict(algorithm = FLANN_INDEX_KDTREE, trees = 5)\n",
    "        search_params = dict(checks=50)   # or pass empty dictionary\n",
    "\n",
    "        flann = cv2.FlannBasedMatcher(index_params, search_params)\n",
    "        matches = flann.knnMatch(des1, des2, k=2)\n",
    "    else:\n",
    "        bf = cv2.BFMatcher()\n",
    "        matches = bf.knnMatch(des1, des2, k=2)\n",
    "\n",
    "    good_matches = []\n",
    "    for m, n in matches:\n",
    "        if m.distance < marg*n.distance:\n",
    "            good_matches.append([m])\n",
    "\n",
    "    x1 = np.stack([kp1[match[0].queryIdx].pt for match in good_matches],1)\n",
    "    x2 = np.stack([kp2[match[0].trainIdx].pt for match in good_matches],1)\n",
    "    x1 = cv.homogenize(x1, multi=True)\n",
    "    x2 = cv.homogenize(x2, multi=True)\n",
    "\n",
    "    des1 = np.stack([des1[match[0].queryIdx] for match in good_matches],0)\n",
    "    des2 = np.stack([des2[match[0].trainIdx] for match in good_matches],0)\n",
    "\n",
    "    if verbose:\n",
    "        print('Number of matches:', np.size(matches,0))\n",
    "        print('Number of good matches:', np.size(x1,1))\n",
    "\n",
    "    return x1, x2, des1, des2\n",
    "\n",
    "def compute_sift_points_TR(x1, des1, img2, marg, flann=False, verbose=False):\n",
    "    img2 = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # nfeatures=0\n",
    "    # sigma=1.6\n",
    "    # contrastThreshold=0.04\n",
    "    # edgeThreshold=10\n",
    "\n",
    "    # nfeatures=nfeatures,\n",
    "    # sigma=2/sigma,  \n",
    "    # contrastThreshold=2/contrastThreshold,\n",
    "    # edgeThreshold=2/edgeThreshold,\n",
    "\n",
    "    sift = cv2.SIFT_create()\n",
    "    kp2, des2 = sift.detectAndCompute(img2, None)\n",
    "\n",
    "    if flann:\n",
    "        FLANN_INDEX_KDTREE = 1\n",
    "        index_params = dict(algorithm = FLANN_INDEX_KDTREE, trees = 5)\n",
    "        search_params = dict(checks=50)   # or pass empty dictionary\n",
    "\n",
    "        flann = cv2.FlannBasedMatcher(index_params, search_params)\n",
    "        matches = flann.knnMatch(des1, des2, k=2)\n",
    "    else:\n",
    "        bf = cv2.BFMatcher()\n",
    "        matches = bf.knnMatch(des1, des2, k=2)\n",
    "\n",
    "    good_matches = []\n",
    "    for m, n in matches:\n",
    "        if m.distance < marg*n.distance:\n",
    "            good_matches.append([m])\n",
    "\n",
    "    x_idx = np.array([match[0].queryIdx for match in good_matches])\n",
    "    x1 = np.stack([x1[:,match[0].queryIdx] for match in good_matches],1)\n",
    "    x2 = np.stack([kp2[match[0].trainIdx].pt for match in good_matches],1)\n",
    "    x2 = cv.homogenize(x2, multi=True)\n",
    "\n",
    "    if verbose:\n",
    "        print('Number of matches:', np.size(matches,0))\n",
    "        print('Number of good matches:', np.size(x1,1))\n",
    "\n",
    "    return x1, x2, x_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set = 1\n",
    "K, img_names, init_pair, pixel_threshold = get_dataset_info(data_set)\n",
    "# pixel_threshold *= 1\n",
    "K_inv = LA.inv(K)\n",
    "imgs = cv.load_image(img_names, multi=True)\n",
    "n_imgs = imgs.shape[0]\n",
    "n_camera_pairs = n_imgs - 1\n",
    "img1_init = imgs[init_pair[0]]\n",
    "img2_init = imgs[init_pair[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cameras_and_3D_points(X, C_arr, axis_arr, s, path, save=False):\n",
    "    \n",
    "    fig = plt.figure(figsize=(8,6))\n",
    "    ax = plt.axes(projection='3d')\n",
    "\n",
    "    ax.plot(X[0], X[1], X[2], '.', ms=0.1, color='magenta', label='3D points')\n",
    "    cv.plot_cameras_and_axes(ax, C_arr, axis_arr, s)\n",
    "\n",
    "    ax.set_xlabel('$x$')\n",
    "    ax.set_ylabel('$y$')\n",
    "    ax.set_zlabel('$z$')\n",
    "    ax.set_aspect('equal')\n",
    "    # ax.view_init(elev=-50, azim=-104, roll=45)\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    if save:\n",
    "        fig.savefig(path, dpi=300)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_feasible_points(P1, P2, X, percentile):\n",
    "    \n",
    "    x1 = P1 @ X\n",
    "    x2 = P2 @ X\n",
    "    x1_filter = x1[-1,:] > 0\n",
    "    x2_filter = x2[-1,:] > 0\n",
    "\n",
    "    X_bar = np.mean(X, axis=1)\n",
    "    X_norm = LA.norm(X - X_bar[:,None], axis=0)\n",
    "    norm_percentile = np.percentile(X_norm, percentile)\n",
    "    outlier_filter = X_norm < norm_percentile\n",
    "\n",
    "    feasible_pts = x1_filter * x2_filter * outlier_filter\n",
    "    return feasible_pts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_absolute_rotations(rel_rots, origin_idx, verbose=False):\n",
    "    \n",
    "    abs_rots = [rel_rots[0]]\n",
    "    for i in range(len(rel_rots)-1):\n",
    "\n",
    "        Ri = abs_rots[i]\n",
    "        R2 = rel_rots[i+1]\n",
    "        if LA.det(R2) < 0:\n",
    "            print('WARNING: det(R{}) < 0, not a rotation!'.format(i), LA.det(R2))\n",
    "            R2 = -R2\n",
    "        U, _, VT = LA.svd(R2, full_matrices=False)\n",
    "        R2 = U @ VT\n",
    "        Rj = R2 @ Ri\n",
    "        abs_rots.append(Rj)\n",
    "\n",
    "    R0 = abs_rots[origin_idx]    \n",
    "    for i in range(len(abs_rots)):\n",
    "\n",
    "        Ri = abs_rots[i]\n",
    "        Ri = LA.inv(R0) @ Ri\n",
    "        abs_rots[i] = Ri\n",
    "\n",
    "        if verbose:\n",
    "            print('det(R{}):'.format(i), LA.det(Ri))\n",
    "        \n",
    "    return np.array(abs_rots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rots_trans_RA(rel_cameras):\n",
    "\n",
    "    P1 = rel_cameras[0]\n",
    "    rots = [P1[:,:-1]]\n",
    "    trans = [P1[:,-1]]\n",
    "\n",
    "    for i in range(rel_cameras.shape[0]-1):\n",
    "        P2 = rel_cameras[i+1]\n",
    "        P2 /= P2[-1,2]\n",
    "        R2 = P2[:,:-1]\n",
    "        T2 = P2[:,-1]\n",
    "\n",
    "        Ri = rots[i]\n",
    "        Ti = trans[i]\n",
    "\n",
    "        Rj = R2 @ Ri\n",
    "        Tj = T2 + (Rj @ Ri.T @ Ti)\n",
    "\n",
    "        rots.append(Rj)\n",
    "        trans.append(Tj)\n",
    "\n",
    "    rots = np.array(rots)\n",
    "    trans = np.array(trans)\n",
    "\n",
    "    return rots, trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "### Computing rotation averaging ###\n",
      "\n",
      "\n",
      "Camera pair: 1 / 8\n",
      "\n",
      "Camera pair: 2 / 8\n",
      "\n",
      "Camera pair: 3 / 8\n",
      "\n",
      "Camera pair: 4 / 8\n",
      "\n",
      "Camera pair: 5 / 8\n",
      "\n",
      "Camera pair: 6 / 8\n",
      "\n",
      "Camera pair: 7 / 8\n",
      "\n",
      "Camera pair: 8 / 8\n"
     ]
    }
   ],
   "source": [
    "print('\\n\\n\\n### Computing rotation averaging ###\\n')\n",
    "\n",
    "marg = 0.75\n",
    "min_its = 0\n",
    "max_its = 50000\n",
    "scale_its = 4\n",
    "alpha = 0.99\n",
    "P1 = cv.get_canonical_camera()\n",
    "rel_cameras = [P1]\n",
    "\n",
    "sift = False\n",
    "ransac = False\n",
    "extract = False\n",
    "plot = False\n",
    "rot = False\n",
    "\n",
    "for i in range(n_camera_pairs):    \n",
    "    print('\\nCamera pair:', i+1, '/', n_camera_pairs)\n",
    "\n",
    "    if sift:\n",
    "        img1 = imgs[i]\n",
    "        img2 = imgs[i+1]\n",
    "        x1, x2, _, _ = compute_sift_points(img1, img2, marg, flann=True, verbose=True)\n",
    "        np.save('data/dataset_{}_RA_x1_{}.npy'.format(data_set, i), x1)\n",
    "        np.save('data/dataset_{}_RA_x2_{}.npy'.format(data_set, i), x2)   \n",
    "\n",
    "    x1 = np.load('data/dataset_{}_RA_x1_{}.npy'.format(data_set, i))\n",
    "    x2 = np.load('data/dataset_{}_RA_x2_{}.npy'.format(data_set, i))\n",
    "    x1_norm = cv.dehomogenize(K_inv @ x1)\n",
    "    x2_norm = cv.dehomogenize(K_inv @ x2)\n",
    "\n",
    "    if ransac:\n",
    "        E, inliers = estimate_E_robust(K, x1_norm, x2_norm, min_its, max_its, scale_its, alpha, pixel_threshold, essential_matrix=True, homography=True, verbose=True)\n",
    "        np.save('data/dataset_{}_RA_E_{}.npy'.format(data_set, i), E)\n",
    "        np.save('data/dataset_{}_RA_E_inliers_{}.npy'.format(data_set, i), inliers)\n",
    "\n",
    "    if extract:\n",
    "        E = np.load('data/dataset_{}_RA_E_{}.npy'.format(data_set, i))\n",
    "        inliers = np.load('data/dataset_{}_RA_E_inliers_{}.npy'.format(data_set, i))\n",
    "\n",
    "        x1_norm_inliers = x1_norm[:,inliers]\n",
    "        x2_norm_inliers = x2_norm[:,inliers]\n",
    "\n",
    "        P2_arr = cv.extract_P_from_E(E)\n",
    "        X_arr = cv.compute_triangulated_X_from_extracted_P2_solutions(P1, P2_arr, x1_norm_inliers, x2_norm_inliers)\n",
    "        P2, X = cv.extract_valid_camera_and_points(P1, P2_arr, X_arr, verbose=True)\n",
    "        rel_cameras.append(P2)\n",
    "\n",
    "        if plot:\n",
    "            percentile = 90\n",
    "            feasable_pts = compute_feasible_points(P1, P2, X, percentile)\n",
    "\n",
    "            P_arr = np.array([P1, P2])\n",
    "            C_arr, axis_arr = cv.compute_camera_center_and_normalized_principal_axis(P_arr, multi=True)\n",
    "            plot_cameras_and_3D_points(X[:,feasable_pts], C_arr, axis_arr, s=1, path=None, save=False)\n",
    "\n",
    "        # x1_norm_filter = x1_norm[:,feasable_pts]\n",
    "        # x2_norm_filter = x2_norm[:,feasable_pts]\n",
    "        # x1s_norm_RA[i] = x1_norm_filter\n",
    "        # x2s_norm_RA[i] = x2_norm_filter\n",
    "        # np.save('data/dataset_{}_RA_x1_{}.npy'.format(data_set, i), x1_norm_filter)\n",
    "        # np.save('data/dataset_{}_RA_x2_{}.npy'.format(data_set, i), x2_norm_filter)\n",
    "\n",
    "if rot:\n",
    "    rel_cameras = np.array(rel_cameras)\n",
    "    rel_rots = rel_cameras[:,:,:-1]\n",
    "    abs_rots = compute_absolute_rotations(rel_rots, init_pair[0], verbose=True)\n",
    "    rots_RA, trans_RA = compute_rots_trans_RA(rel_cameras)\n",
    "\n",
    "    np.save('data/dataset_{}_RA_abs_rots.npy'.format(data_set), abs_rots)\n",
    "    np.save('data/dataset_{}_RA_rots.npy'.format(data_set), rots_RA)\n",
    "    np.save('data/dataset_{}_RA_trans.npy'.format(data_set), trans_RA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "### Computing initial 3D-points ###\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. valid coords for each camera pair: [1780    0  890  890]\n",
      "Argmax(P2_arr): 0\n",
      "(3, 889) (3, 889) (889, 128) (889, 128) (4, 889) (889,)\n"
     ]
    }
   ],
   "source": [
    "print('\\n\\n\\n### Computing initial 3D-points ###\\n')\n",
    "\n",
    "min_its = 100000\n",
    "max_its = 100000\n",
    "scale_its = 3\n",
    "alpha = 0.99\n",
    "\n",
    "sift = False\n",
    "ransac = False\n",
    "extract = True\n",
    "\n",
    "if sift:\n",
    "    marg = 0.75\n",
    "    x1_init, x2_init, des1_init, des2_init = compute_sift_points(imgs[init_pair[0]], imgs[init_pair[1]], marg, flann=True, verbose=True)\n",
    "    np.save('data/dataset_{}_TR_x1_{}.npy'.format(data_set, init_pair[1]), x1_init)\n",
    "    np.save('data/dataset_{}_TR_x2_{}.npy'.format(data_set, init_pair[1]), x2_init)\n",
    "    np.save('data/dataset_{}_TR_des1_init.npy'.format(data_set), des1_init)\n",
    "    np.save('data/dataset_{}_TR_des2_init.npy'.format(data_set), des2_init)\n",
    "\n",
    "x1_init = np.load('data/dataset_{}_TR_x1_{}.npy'.format(data_set, init_pair[1]))\n",
    "x2_init = np.load('data/dataset_{}_TR_x2_{}.npy'.format(data_set, init_pair[1]))\n",
    "x1_init_norm = cv.dehomogenize(K_inv @ x1_init)\n",
    "x2_init_norm = cv.dehomogenize(K_inv @ x2_init)\n",
    "des1_init = np.load('data/dataset_{}_TR_des1_init.npy'.format(data_set))\n",
    "des2_init = np.load('data/dataset_{}_TR_des2_init.npy'.format(data_set))\n",
    "\n",
    "if ransac:\n",
    "    E, inliers = estimate_E_robust(K, x1_init_norm, x2_init_norm, min_its, max_its, scale_its, alpha, pixel_threshold, essential_matrix=True, homography=True, verbose=True)\n",
    "    np.save('data/dataset_{}_TR_E.npy'.format(data_set), E)\n",
    "    np.save('data/dataset_{}_TR_E_inliers.npy'.format(data_set), inliers)\n",
    "\n",
    "if extract:\n",
    "    E = np.load('data/dataset_{}_TR_E.npy'.format(data_set))\n",
    "    inliers = np.load('data/dataset_{}_TR_E_inliers.npy'.format(data_set))\n",
    "    x1_init_norm_inliers = x1_init_norm[:,inliers]\n",
    "    x2_init_norm_inliers = x2_init_norm[:,inliers]\n",
    "    des1_init_inliers = des1_init[inliers]\n",
    "    des2_init_inliers = des2_init[inliers]\n",
    "\n",
    "    P1 = cv.get_canonical_camera()\n",
    "    P2_arr = cv.extract_P_from_E(E)\n",
    "    X_arr = cv.compute_triangulated_X_from_extracted_P2_solutions(P1, P2_arr, x1_init_norm_inliers, x2_init_norm_inliers)\n",
    "    P2, X_init_inliers = cv.extract_valid_camera_and_points(P1, P2_arr, X_arr, verbose=True)\n",
    "\n",
    "    # P_arr = np.array([P1, P2])\n",
    "    # C_arr, axis_arr = cv.compute_camera_center_and_normalized_principal_axis(P_arr, multi=True)\n",
    "    # plot_cameras_and_3D_points(X_init[:,inliers], C_arr, axis_arr, s=1, path=None, save=False)\n",
    "\n",
    "    # R_init = rots[init_pair[0]]\n",
    "    # X_init = LA.inv(R_init) @ X_init[:-1,:]\n",
    "\n",
    "    percentile = 100\n",
    "    feasible_pts = compute_feasible_points(P1, P2, X_init_inliers, percentile)\n",
    "\n",
    "    x1_init_norm_feasible_inliers = x1_init_norm_inliers[:,feasible_pts]\n",
    "    x2_init_norm_feasible_inliers = x2_init_norm_inliers[:,feasible_pts]\n",
    "    des1_init_feasible_inliers = des1_init_inliers[feasible_pts]\n",
    "    des2_init_feasible_inliers = des2_init_inliers[feasible_pts]\n",
    "    X_init_feasible_inliers = X_init_inliers[:,feasible_pts]\n",
    "    \n",
    "    X_init_idx = np.ones(X_init_feasible_inliers.shape[1], dtype=bool)\n",
    "    np.save('data/dataset_{}_TR_x2_norm_{}.npy'.format(data_set, init_pair[0]), x1_init_norm_feasible_inliers)\n",
    "    np.save('data/dataset_{}_TR_x1_norm_{}.npy'.format(data_set, init_pair[1]), x1_init_norm_feasible_inliers)\n",
    "    np.save('data/dataset_{}_TR_x2_norm_{}.npy'.format(data_set, init_pair[1]), x2_init_norm_feasible_inliers)\n",
    "    np.save('data/dataset_{}_TR_X_idx_{}.npy'.format(data_set, init_pair[0]), X_init_idx)\n",
    "    np.save('data/dataset_{}_TR_X_idx_{}.npy'.format(data_set, init_pair[1]), X_init_idx)\n",
    "\n",
    "    print(x1_init_norm_feasible_inliers.shape, x2_init_norm_feasible_inliers.shape, des1_init_feasible_inliers.shape, des2_init_feasible_inliers.shape, X_init_feasible_inliers.shape, X_init_idx.shape)\n",
    "\n",
    "    plot_3D_points(X_init_feasible_inliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_T_robust(K, R, X, x_norm, min_its, max_its, scale_its, alpha, err_threshold_px, DLT1=False, verbose=False):\n",
    "    \n",
    "    err_threshold = err_threshold_px / K[0,0]\n",
    "    best_T = None\n",
    "    best_inliers = None\n",
    "    best_epsilon = 0\n",
    "    n_points = x_norm.shape[1]\n",
    "    n_samples = 2\n",
    "    ransac_its = max_its\n",
    "\n",
    "    t = 0\n",
    "    while t < ransac_its:\n",
    "        t += 1\n",
    "\n",
    "        rand_mask = np.random.choice(n_points, n_samples, replace=False)\n",
    "        if DLT1:\n",
    "            T = cv.estimate_T_DLT_1(x_norm[:,rand_mask], verbose=False)\n",
    "        else:\n",
    "            T = cv.estimate_T_DLT_2(R, x_norm[:,rand_mask], verbose=False)\n",
    "\n",
    "        x_norm_proj = cv.dehomogenize(R @ X + T[:,None])\n",
    "        distance_arr = cv.compute_point_point_distance(x_norm_proj, x_norm)\n",
    "        inliers = distance_arr < err_threshold\n",
    "        n_inliers = np.sum(inliers)\n",
    "        epsilon = n_inliers / n_points\n",
    "\n",
    "        if epsilon > best_epsilon:\n",
    "            best_T = np.copy(T)\n",
    "            best_inliers = np.copy(inliers)\n",
    "            best_epsilon = epsilon\n",
    "            ransac_its = cv.compute_ransac_iterations(alpha, best_epsilon, n_samples, min_its, max_its, scale_its)\n",
    "            if verbose:\n",
    "                print('Iteration:', t, 'T:', ransac_its, 'epsilon:', np.round(best_epsilon, 2), 'No. inliers:', np.sum(inliers))\n",
    "    \n",
    "    print('Bailout at iteration:', t)\n",
    "    return best_T, best_inliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "### Computing translation registrations ###\n",
      "\n",
      "\n",
      "Image: 1 / 9\n",
      "(3, 889) (4, 889)\n",
      "\n",
      "Image: 2 / 9\n",
      "(3, 674) (4, 674)\n",
      "Iteration: 2 T: 5000 epsilon: 0.0 No. inliers: 1\n",
      "Iteration: 17 T: 3630.0 epsilon: 0.04 No. inliers: 24\n",
      "Iteration: 31 T: 3093.0 epsilon: 0.04 No. inliers: 26\n",
      "Iteration: 36 T: 525.0 epsilon: 0.09 No. inliers: 63\n",
      "Iteration: 52 T: 151.0 epsilon: 0.17 No. inliers: 117\n",
      "Bailout at iteration: 151\n",
      "\n",
      "Image: 3 / 9\n",
      "(3, 714) (4, 714)\n",
      "Iteration: 16 T: 5000 epsilon: 0.0 No. inliers: 3\n",
      "Iteration: 22 T: 5000 epsilon: 0.01 No. inliers: 7\n",
      "Iteration: 64 T: 5000 epsilon: 0.02 No. inliers: 13\n",
      "Iteration: 79 T: 3471.0 epsilon: 0.04 No. inliers: 26\n",
      "Iteration: 187 T: 141.0 epsilon: 0.18 No. inliers: 128\n",
      "Bailout at iteration: 187\n",
      "\n",
      "Image: 4 / 9\n",
      "(3, 685) (4, 685)\n",
      "Iteration: 8 T: 5000 epsilon: 0.03 No. inliers: 19\n",
      "Iteration: 19 T: 663.0 epsilon: 0.08 No. inliers: 57\n",
      "Iteration: 347 T: 372.0 epsilon: 0.11 No. inliers: 76\n",
      "Bailout at iteration: 372\n",
      "\n",
      "Image: 5 / 9\n",
      "(3, 677) (4, 677)\n",
      "Iteration: 2 T: 1254.0 epsilon: 0.06 No. inliers: 41\n",
      "Iteration: 41 T: 140.0 epsilon: 0.18 No. inliers: 122\n",
      "Bailout at iteration: 140\n",
      "\n",
      "Image: 6 / 9\n",
      "(3, 663) (4, 663)\n",
      "Iteration: 83 T: 118.0 epsilon: 0.2 No. inliers: 130\n",
      "Bailout at iteration: 118\n",
      "\n",
      "Image: 7 / 9\n",
      "(3, 673) (4, 673)\n",
      "Iteration: 49 T: 13.0 epsilon: 0.55 No. inliers: 368\n",
      "Bailout at iteration: 49\n",
      "\n",
      "Image: 8 / 9\n",
      "(3, 629) (4, 629)\n",
      "Bailout at iteration: 5000\n",
      "\n",
      "Image: 9 / 9\n",
      "(3, 889) (4, 889)\n",
      "Bailout at iteration: 5000\n"
     ]
    }
   ],
   "source": [
    "print('\\n\\n\\n### Computing translation registrations ###\\n')\n",
    "\n",
    "marg = 0.75\n",
    "min_its = 0\n",
    "max_its = 5000\n",
    "scale_its = 1\n",
    "alpha = 0.99\n",
    "trans = []\n",
    "\n",
    "des_X = des1_init_feasible_inliers.copy()\n",
    "x_norm_feasible_inliers = x1_init_norm_feasible_inliers.copy()\n",
    "abs_rots = np.load('data/dataset_{}_RA_abs_rots.npy'.format(data_set))\n",
    "keep_rots = np.ones(abs_rots.shape[0], dtype=bool)\n",
    "\n",
    "sift = False\n",
    "plot = False\n",
    "ransac = True\n",
    "\n",
    "for i in range(n_imgs):\n",
    "    print('\\nImage:', i+1, '/', n_imgs)\n",
    "\n",
    "    if i != init_pair[0]:\n",
    "\n",
    "        if sift and i != init_pair[1]:\n",
    "            img2 = imgs[i]\n",
    "            \n",
    "            # if i < int((init_pair[0]+init_pair[1])/2):\n",
    "            #     des_X = des1_init_feasible_inliers.copy()\n",
    "            #     x_norm_feasible_inliers = x1_init_norm_feasible_inliers.copy()\n",
    "            # else:\n",
    "            #     des_X = des2_init_feasible_inliers.copy()\n",
    "            #     x_norm_feasible_inliers = x2_init_norm_feasible_inliers.copy()\n",
    "            \n",
    "            x1_norm, x2, X_idx = compute_sift_points_TR(x_norm_feasible_inliers, des_X, img2, marg, flann=True, verbose=True)\n",
    "            x2_norm = cv.dehomogenize(K_inv @ x2)\n",
    "\n",
    "            np.save('data/dataset_{}_TR_x1_norm_{}.npy'.format(data_set, i), x1_norm)\n",
    "            np.save('data/dataset_{}_TR_x2_norm_{}.npy'.format(data_set, i), x2_norm)\n",
    "            np.save('data/dataset_{}_TR_X_idx_{}.npy'.format(data_set, i), X_idx)\n",
    "\n",
    "        x_norm = np.load('data/dataset_{}_TR_x2_norm_{}.npy'.format(data_set, i))\n",
    "        X_idx = np.load('data/dataset_{}_TR_X_idx_{}.npy'.format(data_set, i))\n",
    "        X = X_init_feasible_inliers[:,X_idx]        \n",
    "        R = abs_rots[i]\n",
    "        print(x_norm.shape, X.shape)\n",
    "\n",
    "        if plot:\n",
    "            plot_3D_points(X)\n",
    "\n",
    "        if ransac:\n",
    "            T, inliers = estimate_T_robust(K, R, X[:-1], x_norm, min_its, max_its, scale_its, alpha, 50*pixel_threshold, DLT1=False, verbose=True)\n",
    "            # T = cv.estimate_T_DLT_2(R, x_norm[:,inliers], verbose=False)            \n",
    "            np.save('data/dataset_{}_TR_T_inliers_{}.npy'.format(data_set, i), inliers)\n",
    "    else:\n",
    "        x_norm = np.load('data/dataset_{}_TR_x2_norm_{}.npy'.format(data_set, i))\n",
    "        X_idx = np.load('data/dataset_{}_TR_X_idx_{}.npy'.format(data_set, i))\n",
    "        X = X_init_feasible_inliers[:,X_idx]        \n",
    "        print(x_norm.shape, X.shape)\n",
    "\n",
    "        T = np.zeros(3)\n",
    "        inliers = np.ones(x_norm.shape[1], dtype=bool)\n",
    "        np.save('data/dataset_{}_TR_T_inliers_{}.npy'.format(data_set, i), inliers)\n",
    "\n",
    "    \n",
    "    if ransac:\n",
    "        if T is not None:\n",
    "            trans.append(T)\n",
    "        else:\n",
    "            keep_rots[i] = False\n",
    "\n",
    "if ransac:\n",
    "    np.save('data/dataset_{}_TR_trans.npy'.format(data_set), np.array(trans))\n",
    "    np.save('data/dataset_{}_TR_abs_rots.npy'.format(data_set), abs_rots[keep_rots])   \n",
    "    np.save('data/dataset_{}_TR_keep_rots.npy'.format(data_set), keep_rots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "### Refining translation vectors ###\n",
      "\n",
      "\n",
      "Translation: 1 / 7\n",
      "\n",
      "Translation: 2 / 7\n",
      "\n",
      "Translation: 3 / 7\n",
      "\n",
      "Translation: 4 / 7\n",
      "\n",
      "Translation: 5 / 7\n",
      "\n",
      "Translation: 6 / 7\n",
      "\n",
      "Translation: 7 / 7\n"
     ]
    }
   ],
   "source": [
    "def compute_axis_angle_representation(R):\n",
    "    theta = np.arccos((np.trace(R)-1)/2)\n",
    "    a = (1/(2*np.sin(theta))) * np.array([R[2,1]-R[1,2], R[0,2]-R[2,0], R[1,0]-R[0,1]])\n",
    "    w = theta * a\n",
    "    return w\n",
    "\n",
    "def compute_R_matrix_exponential(w, approx=False):\n",
    "    wx = cv.create_skew_symmetric_matrix(w)\n",
    "    w_norm = LA.norm(w)\n",
    "    R = np.eye(3) + (np.sin(w_norm)*wx/w_norm) + ((1-np.cos(w_norm))*(wx @ wx)/(w_norm**2))\n",
    "    if approx:\n",
    "        R = np.eye(3) + wx\n",
    "    return R\n",
    "\n",
    "def compute_residual(params, x, X, R, n_points):\n",
    "    # q = params[0:4]\n",
    "    T = params[0:3]\n",
    "    # X = params[3:].reshape(3, n_points)\n",
    "    \n",
    "    # R = compute_R_matrix_exponential(w, approx=True)\n",
    "    # R = Rotation.from_quat(q).as_matrix()\n",
    "    # U, _, VT = LA.svd(R, full_matrices=False)\n",
    "    # R = U @ VT\n",
    "    x_proj = cv.dehomogenize(R @ X + T[:,None])\n",
    "    residual = x_proj - x\n",
    "\n",
    "    return residual.ravel()\n",
    "\n",
    "\n",
    "\n",
    "print('\\n\\n\\n### Refining translation vectors ###\\n')\n",
    "\n",
    "abs_rots = np.load('data/dataset_{}_TR_abs_rots.npy'.format(data_set))\n",
    "trans = np.load('data/dataset_{}_TR_trans.npy'.format(data_set))\n",
    "\n",
    "abs_rots_opt = []\n",
    "trans_opt = []\n",
    "\n",
    "RA = False\n",
    "\n",
    "if RA:\n",
    "    abs_rots = np.load('data/dataset_{}_RA_rots.npy'.format(data_set))\n",
    "    trans = np.load('data/dataset_{}_RA_trans.npy'.format(data_set))\n",
    "\n",
    "for i in range(trans.shape[0]):\n",
    "    \n",
    "    print('\\nTranslation:', i+1, '/', trans.shape[0])\n",
    "\n",
    "    if i != init_pair[0]:\n",
    " \n",
    "        x_norm = np.load('data/dataset_{}_TR_x2_norm_{}.npy'.format(data_set, i))\n",
    "        X_idx = np.load('data/dataset_{}_TR_X_idx_{}.npy'.format(data_set, i))\n",
    "        if RA:\n",
    "            inliers_T = np.ones(x_norm.shape[1], dtype=bool)\n",
    "        else:\n",
    "            inliers_T = np.load('data/dataset_{}_TR_T_inliers_{}.npy'.format(data_set, i))\n",
    "        \n",
    "        x_norm = x_norm[:,inliers_T]\n",
    "        X = X_init_feasible_inliers[:,X_idx][:,inliers_T]\n",
    "        R = abs_rots[i]\n",
    "        n_points = x_norm.shape[1]\n",
    "        T = trans[i]\n",
    "        x0 = T.ravel()\n",
    "        result = scipy.optimize.least_squares(compute_residual, x0, method='lm', args=(x_norm, X[:-1], R, n_points))\n",
    "        T_opt = result.x\n",
    "\n",
    "        trans_opt.append(T_opt)\n",
    "    else:\n",
    "        trans_opt.append(trans[i])\n",
    "        \n",
    "np.save('data/dataset_{}_LM_trans_opt.npy'.format(data_set), np.array(trans_opt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Iteration     Total nfev        Cost      Cost reduction    Step norm     Optimality   \n",
      "       0              1         1.0645e-01                                    2.81e+00    \n",
      "       1              2         5.1294e-02      5.52e-02       5.66e+01       5.87e-01    \n",
      "       2              3         2.5301e-03      4.88e-02       5.66e+01       1.19e-01    \n",
      "       3              4         1.8170e-04      2.35e-03       1.13e+02       2.27e-02    \n",
      "       4              6         1.6322e-04      1.85e-05       5.66e+01       4.11e-02    \n",
      "       5              7         2.1143e-05      1.42e-04       1.42e+01       2.28e-03    \n",
      "       6              8         2.0904e-05      2.38e-07       2.83e+01       2.16e-03    \n",
      "       7             10         2.0790e-05      1.14e-07       7.08e+00       1.05e-03    \n",
      "       8             11         2.0774e-05      1.66e-08       1.42e+01       7.40e-05    \n",
      "       9             12         2.0774e-05      6.69e-11       2.83e+01       1.82e-05    \n",
      "`ftol` termination condition is satisfied.\n",
      "Function evaluations 12, initial cost 1.0645e-01, final cost 2.0774e-05, first-order optimality 1.82e-05.\n"
     ]
    }
   ],
   "source": [
    "def fun(params, n_trans, n_points, abs_rots, keep_rots, xs_norm):\n",
    "\n",
    "    trans = params[:n_trans * 3].reshape((n_trans, 3))\n",
    "    X_init = params[n_trans * 3:].reshape((3, n_points))\n",
    "\n",
    "    xs_proj = []\n",
    "    for i in range(n_trans):\n",
    "        if keep_rots[i]:\n",
    "\n",
    "            X_idx = np.load('data/dataset_{}_TR_X_idx_{}.npy'.format(data_set, i))\n",
    "            inliers_T = np.load('data/dataset_{}_TR_T_inliers_{}.npy'.format(data_set, i))\n",
    "\n",
    "            R = abs_rots[i] \n",
    "            X = X_init[:,X_idx][:,inliers_T]\n",
    "            T = trans[i]\n",
    "\n",
    "            x_proj = cv.dehomogenize(R @ X + T[:,None])\n",
    "            xs_proj.append(x_proj)\n",
    "    xs_proj = np.concatenate(xs_proj, 1)\n",
    "\n",
    "    return (xs_proj - xs_norm).ravel()\n",
    "\n",
    "abs_rots = np.load('data/dataset_{}_TR_abs_rots.npy'.format(data_set))\n",
    "trans = np.load('data/dataset_{}_TR_trans.npy'.format(data_set))\n",
    "keep_rots = np.load('data/dataset_{}_TR_keep_rots.npy'.format(data_set))\n",
    "\n",
    "xs_norm = []\n",
    "for i in range(trans.shape[0]):\n",
    "    if keep_rots[i]:\n",
    "        x_norm = np.load('data/dataset_{}_TR_x2_norm_{}.npy'.format(data_set, i))\n",
    "        inliers_T = np.load('data/dataset_{}_TR_T_inliers_{}.npy'.format(data_set, i))\n",
    "        xs_norm.append(x_norm[:,inliers_T])\n",
    "xs_norm = np.concatenate(xs_norm, 1)\n",
    "\n",
    "n_trans = trans.shape[0]\n",
    "n_points = X_init_feasible_inliers.shape[1]\n",
    "\n",
    "x0 = np.hstack((trans.ravel(), X_init_feasible_inliers[:-1].ravel()))\n",
    "res = scipy.optimize.least_squares(fun, x0, verbose=2, ftol=1e-4, method='trf', args=(n_trans, n_points, abs_rots, keep_rots, xs_norm))\n",
    "\n",
    "trans_opt = res.x[:n_trans * 3].reshape((n_trans, 3))\n",
    "np.save('data/dataset_{}_LM_trans_opt.npy'.format(data_set), np.array(trans_opt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. cameras: 7\n"
     ]
    }
   ],
   "source": [
    "cameras = []\n",
    "abs_rots = np.load('data/dataset_{}_TR_abs_rots.npy'.format(data_set))\n",
    "trans = np.load('data/dataset_{}_TR_trans.npy'.format(data_set))\n",
    "\n",
    "RA = False\n",
    "opt = True\n",
    "\n",
    "if RA:\n",
    "    abs_rots = np.load('data/dataset_{}_RA_rots.npy'.format(data_set))\n",
    "    trans = np.load('data/dataset_{}_RA_trans.npy'.format(data_set))\n",
    "\n",
    "if opt:\n",
    "    trans = np.load('data/dataset_{}_LM_trans_opt.npy'.format(data_set))\n",
    "\n",
    "for i in range(len(trans)):\n",
    "    try:\n",
    "        T = trans[i]\n",
    "        R = abs_rots[i]\n",
    "        cameras.append(np.column_stack((R, T)))\n",
    "    except:\n",
    "        break\n",
    "    \n",
    "cameras = np.array(cameras)\n",
    "print('No. cameras:', cameras.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "### Triangulating final 3D-reconstruction ###\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('\\n\\n\\n### Triangulating final 3D-reconstruction ###\\n')\n",
    "\n",
    "X_final = []\n",
    "reconstruction = True\n",
    "\n",
    "if reconstruction:\n",
    "    for i in range(len(cameras)-1):\n",
    "        P1 = cameras[i]\n",
    "        P2 = cameras[i+1]\n",
    "\n",
    "        x1 = np.load('data/dataset_{}_RA_x1_{}.npy'.format(data_set, i))\n",
    "        x2 = np.load('data/dataset_{}_RA_x2_{}.npy'.format(data_set, i))\n",
    "        x1_norm = cv.dehomogenize(K_inv @ x1)\n",
    "        x2_norm = cv.dehomogenize(K_inv @ x2)\n",
    "        \n",
    "        inliers = np.load('data/dataset_{}_RA_E_inliers_{}.npy'.format(data_set, i))\n",
    "        x1_norm_inliers = x1_norm[:,inliers]\n",
    "        x2_norm_inliers = x2_norm[:,inliers]\n",
    "\n",
    "        percentile = 90\n",
    "        X_inliers = cv.triangulate_3D_point_DLT(P1, P2, x1_norm_inliers, x2_norm_inliers, verbose=False)\n",
    "        feasible_pts = compute_feasible_points(P1, P2, X_inliers, percentile)\n",
    "        X_final.append(X_inliers[:,feasible_pts])\n",
    "\n",
    "    X_final = np.column_stack(X_final)\n",
    "else:\n",
    "    X_final = [0,0,0]\n",
    "\n",
    "C_arr, axis_arr = cv.compute_camera_center_and_normalized_principal_axis(cameras, multi=True)\n",
    "plot_cameras_and_3D_points(X_final, C_arr, axis_arr, s=1, path=None, save=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_chalmers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
