{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import linalg as LA\n",
    "from scipy.io import loadmat\n",
    "from mpl_toolkits import mplot3d\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from matplotlib.pyplot import cm\n",
    "import matplotlib as mpl\n",
    "import cv2\n",
    "import computer_vision as cv\n",
    "from icecream import ic\n",
    "from tqdm import trange\n",
    "import time\n",
    "from get_dataset_info import *\n",
    "\n",
    "# %load_ext snakeviz\n",
    "# %matplotlib inline\n",
    "%matplotlib qt\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "from matplotlib import rc\n",
    "rc('font', **{'family': 'serif', 'serif': ['Computer Modern']})\n",
    "rc('text', usetex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_E_robust_old(K, x1_norm, x2_norm, n_its, n_samples, err_threshold_px, alpha):\n",
    "    \n",
    "    err_threshold = err_threshold_px / K[0,0]\n",
    "    best_inliers = None\n",
    "    best_E = None\n",
    "    max_inliers = 0\n",
    "    epsilon = 0\n",
    "    T = n_its\n",
    "    n_points = x1_norm.shape[1]\n",
    "\n",
    "    for t in trange(n_its):\n",
    "\n",
    "        rand_mask = np.random.choice(np.size(x1_norm,1), n_samples, replace=False)\n",
    "        E = cv.estimate_E_DLT(x1_norm[:,rand_mask], x2_norm[:,rand_mask], enforce=True, verbose=False)\n",
    "\n",
    "        D1, D2 = cv.compute_epipolar_errors(E, x1_norm, x2_norm)\n",
    "        inliers = ((D1**2 + D2**2) / 2) < err_threshold**2\n",
    "\n",
    "        n_inliers = np.sum(inliers)\n",
    "\n",
    "        if n_inliers > max_inliers:\n",
    "            best_inliers = np.copy(inliers)\n",
    "            best_E = np.copy(E)\n",
    "            max_inliers = n_inliers\n",
    "            print('No. inliers:', np.sum(inliers), end='\\r')\n",
    "\n",
    "            # Extract R and T\n",
    "\n",
    "            # epsilon = max_inliers / n_points\n",
    "            # T = cv.compute_ransac_iterations(alpha, epsilon, n_samples)\n",
    "            # print('New T:', T, 'New epsilon:', epsilon)\n",
    "            # if t >= 4*T-1:\n",
    "            #     print('Bailout at iteration:', t, T)\n",
    "            #     break\n",
    "        \n",
    "    return best_E, best_inliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_valid_inliers(P1, P2, X, inliers):\n",
    "\n",
    "    x1_norm_valid = P1 @ X\n",
    "    x2_norm_valid = P2 @ X\n",
    "    valid_coords_P1 = x1_norm_valid[-1,:] > 0\n",
    "    valid_coords_P2 = x2_norm_valid[-1,:] > 0\n",
    "    valid_coords = valid_coords_P1 * valid_coords_P2\n",
    "    valid_inliers = inliers * valid_coords\n",
    "\n",
    "    return valid_inliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_H_DLT(img1_pts, img2_pts, verbose=False):\n",
    "\n",
    "    n = np.size(img1_pts,1)\n",
    "    M = []\n",
    "\n",
    "    for i in range(n):\n",
    "\n",
    "        x = img1_pts[0,i]\n",
    "        y = img1_pts[1,i]\n",
    "\n",
    "        u = img2_pts[0,i]\n",
    "        v = img2_pts[1,i]\n",
    "\n",
    "        m = np.array([[x, y, 1, 0, 0, 0, -u*x, -u*y, -u],\n",
    "                      [0, 0, 0, x, y, 1, -v*x, -v*y, -v]])\n",
    "\n",
    "        M.append(m)\n",
    "\n",
    "    M = np.concatenate(M, 0)\n",
    "    U, S, VT = LA.svd(M, full_matrices=False)\n",
    "    H = np.stack([VT[-1, i:i+3] for i in range(0, 9, 3)], 0)\n",
    "\n",
    "    if verbose:\n",
    "        M_approx = U @ np.diag(S) @ VT\n",
    "        v = VT[-1,:] # last row of VT because optimal v should be last column of V\n",
    "        Mv = M @ v\n",
    "        print('\\n||Mv||:', (Mv @ Mv)**0.5)\n",
    "        print('||v||^2:', v @ v)\n",
    "        print('max{||M - M_approx||}:', np.max(np.abs(M - M_approx)))\n",
    "        print('S:', S)\n",
    "\n",
    "    return H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def homography_to_RT(H, x1, x2):\n",
    "    def unitize(a, b):\n",
    "        denom = 1.0 / np.sqrt(a**2 + b**2)\n",
    "        ra = a * denom\n",
    "        rb = b * denom\n",
    "        return ra, rb\n",
    "\n",
    "    # Check the right sign for H\n",
    "    if LA.det(H) < 0:\n",
    "        H *= -1 \n",
    "        \n",
    "    N = x1.shape[1]\n",
    "    if x1.shape[0] != 3:\n",
    "        x1 = np.vstack([x1, np.ones((1, N))])\n",
    "    if x2.shape[0] != 3:\n",
    "        x2 = np.vstack([x2, np.ones((1, N))])\n",
    "\n",
    "    positives = np.sum(np.sum(x2 * (H @ x1), axis=0) > 0)\n",
    "    if positives < (N / 2):\n",
    "        H *= -1\n",
    "\n",
    "    U, S, VT = np.linalg.svd(H, full_matrices=False)\n",
    "    V = VT.T\n",
    "    s1 = S[0] / S[1]\n",
    "    s3 = S[2] / S[1]\n",
    "    zeta = s1 - s3\n",
    "    a1 = np.sqrt(1 - s3**2)\n",
    "    b1 = np.sqrt(s1**2 - 1)\n",
    "    a, b = unitize(a1, b1)\n",
    "    c, d = unitize(1+s1*s3, a1*b1)\n",
    "    e, f = unitize(-b/s1, -a/s3)\n",
    "    v1, v3 = V[:, 0], V[:, 2]\n",
    "    n1 = b * v1 - a * v3\n",
    "    n2 = b * v1 + a * v3\n",
    "    R1 = U @ np.array([[c, 0, d], [0, 1, 0], [-d, 0, c]]) @ VT\n",
    "    R2 = U @ np.array([[c, 0, -d], [0, 1, 0], [d, 0, c]]) @ VT\n",
    "    t1 = e * v1 + f * v3\n",
    "    t2 = e * v1 - f * v3\n",
    "    if n1[2] < 0:\n",
    "        t1 = -t1\n",
    "        n1 = -n1\n",
    "    if n2[2] < 0:\n",
    "        t2 = -t2\n",
    "        n2 = -n2\n",
    "\n",
    "    # Move from Triggs' convention H = R*(I - t*n') to H&Z notation H = R - t*n'\n",
    "    t1 = R1 @ t1\n",
    "    t2 = R2 @ t2\n",
    "\n",
    "    # Verify that we obtain the initial homography back\n",
    "    # H /= S[1]\n",
    "    # print(np.linalg.norm(R1 - zeta * np.outer(t1, n1) - H), np.linalg.norm(R2 - zeta * np.outer(t2, n2) - H))\n",
    "\n",
    "    return R1, t1, R2, t2\n",
    "\n",
    "# Example usage:\n",
    "# H is the homography matrix, x1 and x2 are the corresponding 2D points\n",
    "# R1, t1, R2, t2 = homography_to_RT(H, x1, x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_point_point_distance(x_proj, x_img):\n",
    "    distance_arr = LA.norm(x_proj - x_img, axis=0)\n",
    "    return distance_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# P1 = cv.get_canonical_camera()\n",
    "# best_P2 = None\n",
    "# best_X = None\n",
    "# max_inliers = 0\n",
    "# if epsilon > epsilon_E:\n",
    "\n",
    "#     # P2_arr = cv.extract_P_from_E(E)\n",
    "#     # X_arr = cv.compute_triangulated_X_from_extracted_P2_solutions(P1, P2_arr, x1_norm, x2_norm)\n",
    "#     # P2_valid, X_valid = cv.extract_valid_camera_and_points(P1, P2_arr, X_arr)\n",
    "#     # valid_inliers = compute_valid_inliers(P1, P2_valid, X_valid, inliers)\n",
    "#     # n_valid_inliers = np.sum(valid_inliers)\n",
    "#     # print(n_valid_inliers, n_inliers)\n",
    "\n",
    "#     # best_P2 = np.copy(P2_valid)\n",
    "#     # best_X = np.copy(X_valid)\n",
    "#     best_E = np.copy(E)\n",
    "#     best_inliers = np.copy(inliers)\n",
    "#     epsilon_E = epsilon\n",
    "#     print('No. inliers:', n_inliers, end='\\r')\n",
    "\n",
    "#     # if n_valid_inliers > max_inliers:\n",
    "#     #     best_inliers = np.copy(valid_inliers)\n",
    "#     #     best_P2 = np.copy(P2_valid)\n",
    "#     #     max_inliers = n_valid_inliers\n",
    "#     #     print('No. valid inliers:', n_valid_inliers, end='\\r')\n",
    "\n",
    "#         # epsilon = max_inliers / n_points\n",
    "#         # T = cv.compute_ransac_iterations(alpha, epsilon, n_samples)\n",
    "#         # print('New T:', T, 'New epsilon:', epsilon)\n",
    "#         # if t >= 4*T-1:\n",
    "#         #     print('Bailout at iteration:', t, T)\n",
    "#         #     break\n",
    "\n",
    "# valid_inliers = compute_valid_inliers(P1, best_P2, best_X, best_inliers)\n",
    "# n_valid_inliers = np.sum(valid_inliers)\n",
    "# print('No. valid inliers:', n_valid_inliers, 'No. inliers:', np.sum(best_inliers))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_E_validity(E):\n",
    "    rank = LA.matrix_rank(E)\n",
    "    valid = True if rank == 2 else False\n",
    "    return valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_E_inliers(E, x1_norm, x2_norm, err_threshold):\n",
    "    \n",
    "    distance1_arr, distance2_arr = cv.compute_epipolar_errors(E, x1_norm, x2_norm)\n",
    "    inliers = ((distance1_arr**2 + distance2_arr**2) / 2) < err_threshold**2\n",
    "    n_inliers = np.sum(inliers)\n",
    "    epsilon_E = n_inliers / x1_norm.shape[1]\n",
    "\n",
    "    return epsilon_E, inliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verbose_E_robust(t, T_E, T_H, epsilon_E, epsilon_H, inliers, method):\n",
    "    print('Iteration:', t, 'T_E:', T_E, 'T_H:', T_H, 'epsilon_E:', np.round(epsilon_E, 2), 'epsilon_H:', np.round(epsilon_H, 2), 'No. inliers:', np.sum(inliers), 'From:', method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_E_robust(K, x1_norm, x2_norm, min_its, max_its, scale_its, alpha, err_threshold_px, verbose=False):\n",
    "    \n",
    "    err_threshold = err_threshold_px / K[0,0]\n",
    "    best_E = None\n",
    "    best_inliers = None\n",
    "    n_points = x1_norm.shape[1]\n",
    "    n_E_samples = 8\n",
    "    n_H_samples = 4\n",
    "    best_epsilon_E = 0\n",
    "    best_epsilon_H = 0\n",
    "    T_E = max_its\n",
    "    T_H = max_its\n",
    "\n",
    "    t = 0\n",
    "    while t < T_E and t < T_H:\n",
    "        t += 1\n",
    "\n",
    "        rand_mask = np.random.choice(n_points, n_E_samples, replace=False)\n",
    "        E = cv.estimate_E_DLT(x1_norm[:,rand_mask], x2_norm[:,rand_mask], enforce=True, verbose=False)\n",
    "        E_valid = compute_E_validity(E)\n",
    "\n",
    "        if E_valid:\n",
    "            epsilon_E, inliers = compute_E_inliers(E, x1_norm, x2_norm, err_threshold)\n",
    "                \n",
    "            if epsilon_E > best_epsilon_E:\n",
    "                best_E = np.copy(E)\n",
    "                best_inliers = np.copy(inliers)\n",
    "                best_epsilon_E = epsilon_E\n",
    "                T_E = cv.compute_ransac_iterations(alpha, best_epsilon_E, n_E_samples, min_its, max_its, scale_its)\n",
    "\n",
    "                if verbose:\n",
    "                    verbose_E_robust(t, T_E, T_H, best_epsilon_E, best_epsilon_H, best_inliers, method='E 8-point alg.')\n",
    "        \n",
    "        rand_mask = np.random.choice(n_points, n_H_samples, replace=False)\n",
    "        H = estimate_H_DLT(x1_norm[:,rand_mask], x2_norm[:,rand_mask], verbose=False)\n",
    "        x2_norm_proj = cv.dehomogenize(H @ x1_norm)\n",
    "        distance_arr = compute_point_point_distance(x2_norm_proj, x2_norm)\n",
    "        inliers = distance_arr**2 < err_threshold**2\n",
    "        n_inliers = np.sum(inliers)\n",
    "        epsilon_H = n_inliers / n_points\n",
    "\n",
    "        if epsilon_H > best_epsilon_H:\n",
    "            \n",
    "            # num, Rs, Ts, Ns = cv2.decomposeHomographyMat(H, np.eye(3))\n",
    "            R1, T1, R2, T2 = homography_to_RT(H, x1_norm, x2_norm)\n",
    "            E1 = cv.compute_E_from_R_and_T(R1, T1)\n",
    "            E2 = cv.compute_E_from_R_and_T(R2, T2)\n",
    "\n",
    "            E1_valid = compute_E_validity(E1)\n",
    "            E2_valid = compute_E_validity(E2)\n",
    "\n",
    "            if E1_valid:\n",
    "                epsilon_E, inliers = compute_E_inliers(E1, x1_norm, x2_norm, err_threshold)\n",
    "                    \n",
    "                if epsilon_E > best_epsilon_E:\n",
    "                    best_E = np.copy(E1)\n",
    "                    best_inliers = np.copy(inliers)\n",
    "                    best_epsilon_E = epsilon_E\n",
    "                    best_epsilon_H = epsilon_H\n",
    "                    T_E = cv.compute_ransac_iterations(alpha, best_epsilon_E, n_E_samples, min_its, max_its, scale_its)\n",
    "                    T_H = cv.compute_ransac_iterations(alpha, best_epsilon_H, n_H_samples, min_its, max_its, scale_its)\n",
    "\n",
    "                    if verbose:\n",
    "                        verbose_E_robust(t, T_E, T_H, best_epsilon_E, best_epsilon_H, best_inliers, method='H 4-point alg.')\n",
    "\n",
    "            if E2_valid:\n",
    "                epsilon_E, inliers = compute_E_inliers(E2, x1_norm, x2_norm, err_threshold)\n",
    "                    \n",
    "                if epsilon_E > best_epsilon_E:\n",
    "                    best_E = np.copy(E2)\n",
    "                    best_inliers = np.copy(inliers)\n",
    "                    best_epsilon_E = epsilon_E\n",
    "                    best_epsilon_H = epsilon_H\n",
    "                    T_E = cv.compute_ransac_iterations(alpha, best_epsilon_E, n_E_samples, min_its, max_its, scale_its)\n",
    "                    T_H = cv.compute_ransac_iterations(alpha, best_epsilon_H, n_H_samples, min_its, max_its, scale_its)\n",
    "                    \n",
    "                    if verbose:\n",
    "                        verbose_E_robust(t, T_E, T_H, best_epsilon_E, best_epsilon_H, best_inliers, method='H 4-point alg.')\n",
    "    \n",
    "    print('Bailout at iteration:', t)\n",
    "    return best_E, best_inliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set = 0\n",
    "K, img_names, init_pair, pixel_threshold = get_dataset_info(data_set)\n",
    "K_inv = LA.inv(K)\n",
    "imgs = cv.load_image(img_names, multi=True)\n",
    "n_imgs = imgs.shape[0]\n",
    "n_camera_pairs = n_imgs - 1\n",
    "img1_init = imgs[init_pair[0]]\n",
    "img2_init = imgs[init_pair[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and save SIFT points\n",
    "sift = False\n",
    "marg = 0.7\n",
    "\n",
    "if sift:\n",
    "    \n",
    "    # SIFT points for rotation averaging\n",
    "    for i in range(n_camera_pairs):\n",
    "        print(\"\\nCamera pair:\", i+1, \"/\", n_camera_pairs)\n",
    "        img1 = imgs[i]\n",
    "        img2 = imgs[i+1]\n",
    "        x1, x2, _, _, _, _, _ = cv.compute_sift_points(img1, img2, marg, verbose=True)\n",
    "        np.save('data/dataset_{}_RA_x1_{}.npy'.format(data_set, i), x1)\n",
    "        np.save('data/dataset_{}_RA_x2_{}.npy'.format(data_set, i), x2)\n",
    "        \n",
    "\n",
    "    # SIFT points for translation registration\n",
    "    x1, x2, kp1, kp2, des1, des2, _ = cv.compute_sift_points(img1_init, img2_init, marg, verbose=True)\n",
    "    np.save('data/dataset_{}_TR_x1_{}.npy'.format(data_set, init_pair[1]), x1)\n",
    "    np.save('data/dataset_{}_TR_x2_{}.npy'.format(data_set, init_pair[1]), x2)\n",
    "\n",
    "    for i in range(n_imgs):\n",
    "\n",
    "        if i != init_pair[0] and i != init_pair[1]:\n",
    "            \n",
    "            print(\"\\nImage:\", i+1, \"/\", n_imgs)\n",
    "            img2 = imgs[i]\n",
    "            x1, x2 = cv.compute_sift_points_sequential(kp1, des1, img2, marg, verbose=True)\n",
    "            np.save('data/dataset_{}_TR_x1_{}.npy'.format(data_set, i), x1)\n",
    "            np.save('data/dataset_{}_TR_x2_{}.npy'.format(data_set, i), x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(44542, 128) (47937, 128) 44542 47937\n",
      "(14019, 128) (14019, 128)\n",
      "[ 16.   5.   0.   1.  97. 102.   5.  10. 104.  14.   0.   0.   8.   9.\n",
      "   4. 103.  49.  14.   0.   1.  32.  24.   8.  62.  11.   9.  13.   4.\n",
      "  44.  34.  10.  25.  40.  24.   5.  43. 120.  10.   1.   4. 120.  32.\n",
      "   6.   7.  20.   2.   2.  74.  44.  15.  31.  38.  29.  32.   9.  45.\n",
      "   4.   8.  16.  10.  67.  79.   3.   5.  13.  11.   2.  25.  95. 106.\n",
      "  16.   6.  90. 120.  57.  10.  18.  13.   0.   2.  63.  70. 114.  57.\n",
      "   2.   1.   0.  40.  43.  17.  10.   7.  53.  44.   2.  30.   0.   0.\n",
      "   0.   0.  84. 120.  13.   1.  37.  30.  11.   2.  59. 102.   1.   1.\n",
      " 118.  86.  20.   0.   0.   2.   1.  38.  15.  52.  20.   1.  31.  16.\n",
      "   0.  18.]\n",
      "Number of matches: 44542\n",
      "Number of good matches: 14019\n"
     ]
    }
   ],
   "source": [
    "def compute_sift_points(img1, img2, marg, verbose=False):\n",
    "    img1 = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)\n",
    "    img2 = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    sift = cv2.SIFT_create()\n",
    "    kp1, des1 = sift.detectAndCompute(img1, None)\n",
    "    kp2, des2 = sift.detectAndCompute(img2, None)\n",
    "\n",
    "    FLANN_INDEX_KDTREE = 1\n",
    "    index_params = dict(algorithm = FLANN_INDEX_KDTREE, trees = 5)\n",
    "    search_params = dict(checks=50)   # or pass empty dictionary\n",
    "\n",
    "    flann = cv2.FlannBasedMatcher(index_params, search_params)\n",
    "    matches = flann.knnMatch(des1, des2, k=2)\n",
    "\n",
    "    good_matches = []\n",
    "    for m, n in matches:\n",
    "        if m.distance < marg*n.distance:\n",
    "            good_matches.append([m])\n",
    "\n",
    "    draw_params = dict(matchColor=(255,0,255), singlePointColor=(0,255,0), matchesMask=None, flags=cv2.DrawMatchesFlags_DEFAULT)\n",
    "    img_match = cv2.drawMatchesKnn(img1, kp1, img2, kp2, good_matches, None, **draw_params)\n",
    "\n",
    "    x1 = np.stack([kp1[match[0].queryIdx].pt for match in good_matches],1)\n",
    "    x2 = np.stack([kp2[match[0].trainIdx].pt for match in good_matches],1)\n",
    "    x1 = cv.homogenize(x1, multi=True)\n",
    "    x2 = cv.homogenize(x2, multi=True)\n",
    "\n",
    "    des1 = np.stack([des1[match[0].queryIdx] for match in good_matches],0)\n",
    "    des2 = np.stack([des2[match[0].trainIdx] for match in good_matches],0)\n",
    "\n",
    "    if verbose:\n",
    "        print('Number of matches:', np.size(matches,0))\n",
    "        print('Number of good matches:', np.size(x1,1))\n",
    "\n",
    "    return x1, x2,  des1, des2, img_match\n",
    "\n",
    "\n",
    "x1, x2, des1, des2, _ = compute_sift_points(img1_init, img2_init, 0.6, verbose=True)\n",
    "\n",
    "np.save('data/dataset_{}_TR_x1_{}.npy'.format(data_set, init_pair[1]), x1)\n",
    "np.save('data/dataset_{}_TR_x2_{}.npy'.format(data_set, init_pair[1]), x2)\n",
    "\n",
    "for i in range(n_imgs):\n",
    "\n",
    "    if i != init_pair[0] and i != init_pair[1]:\n",
    "        \n",
    "        print(\"\\nImage:\", i+1, \"/\", n_imgs)\n",
    "        img2 = imgs[i]\n",
    "        x1, x2 = cv.compute_sift_points_sequential(kp1, des1, img2, marg, verbose=True)\n",
    "        np.save('data/dataset_{}_TR_x1_{}.npy'.format(data_set, i), x1)\n",
    "        np.save('data/dataset_{}_TR_x2_{}.npy'.format(data_set, i), x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load SIFT points\n",
    "\n",
    "# SIFT points for rotation averaging\n",
    "x1s_norm_RA = []\n",
    "x2s_norm_RA = []\n",
    "\n",
    "for i in range(n_camera_pairs):\n",
    "\n",
    "    x1 = np.load('data/dataset_{}_RA_x1_{}.npy'.format(data_set, i))\n",
    "    x2 = np.load('data/dataset_{}_RA_x2_{}.npy'.format(data_set, i))\n",
    "    x1_norm = cv.dehomogenize(K_inv @ x1)\n",
    "    x2_norm = cv.dehomogenize(K_inv @ x2)\n",
    "    x1s_norm_RA.append(x1_norm)\n",
    "    x2s_norm_RA.append(x2_norm)\n",
    "\n",
    "x1s_norm_RA = np.array(x1s_norm_RA)\n",
    "x2s_norm_RA = np.array(x2s_norm_RA)\n",
    "\n",
    "\n",
    "# SIFT points for translation registration\n",
    "x1s_norm_TR = []\n",
    "x2s_norm_TR = []\n",
    "\n",
    "for i in range(n_imgs):\n",
    "\n",
    "    if i != init_pair[0]:\n",
    "\n",
    "        x1 = np.load('data/dataset_{}_TR_x1_{}.npy'.format(data_set, i))\n",
    "        x2 = np.load('data/dataset_{}_TR_x2_{}.npy'.format(data_set, i))\n",
    "        x1_norm = cv.dehomogenize(K_inv @ x1)\n",
    "        x2_norm = cv.dehomogenize(K_inv @ x2)\n",
    "        x1s_norm_TR.append(x1_norm)\n",
    "        x2s_norm_TR.append(x2_norm)\n",
    "\n",
    "x1s_norm_TR = np.array(x1s_norm_TR)\n",
    "x2s_norm_TR = np.array(x2s_norm_TR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1 T_E: 10000 T_H: 10000 epsilon_E: 0.05 epsilon_H: 0 No. inliers: 792 From: E 8-point alg.\n",
      "Iteration: 9 T_E: 10000 T_H: 10000 epsilon_E: 0.06 epsilon_H: 0.01 No. inliers: 970 From: H 4-point alg.\n",
      "Iteration: 9 T_E: 10000 T_H: 10000 epsilon_E: 0.11 epsilon_H: 0.01 No. inliers: 1828 From: H 4-point alg.\n",
      "Iteration: 449 T_E: 10000 T_H: 10000 epsilon_E: 0.14 epsilon_H: 0.02 No. inliers: 2312 From: H 4-point alg.\n",
      "Iteration: 534 T_E: 10000 T_H: 10000 epsilon_E: 0.19 epsilon_H: 0.02 No. inliers: 3134 From: E 8-point alg.\n",
      "Iteration: 3959 T_E: 10000 T_H: 10000 epsilon_E: 0.21 epsilon_H: 0.02 No. inliers: 3444 From: E 8-point alg.\n",
      "Iteration: 4734 T_E: 10000 T_H: 10000 epsilon_E: 0.31 epsilon_H: 0.02 No. inliers: 5187 From: E 8-point alg.\n",
      "Iteration: 8681 T_E: 2110.0 T_H: 10000 epsilon_E: 0.44 epsilon_H: 0.02 No. inliers: 7273 From: E 8-point alg.\n",
      "Bailout at iteration: 8681\n",
      "No. valid coords for each camera pair: [    0 14546  7273  7273]\n",
      "Argmax(P2_arr): 1\n"
     ]
    }
   ],
   "source": [
    "# Compute rotation averaging\n",
    "\n",
    "min_its = 0\n",
    "max_its = 10000\n",
    "scale_its = 1\n",
    "alpha = 0.95\n",
    "P1 = cv.get_canonical_camera()\n",
    "abs_rots = [P1[:,:-1]]\n",
    "abs_trans = [P1[:,-1]]\n",
    "\n",
    "for i in range(n_camera_pairs):    \n",
    "    \n",
    "    x1_norm = x1s_norm_RA[i]\n",
    "    x2_norm = x2s_norm_RA[i]\n",
    "    E, inliers = estimate_E_robust(K, x1_norm, x2_norm, min_its, max_its, scale_its, alpha, pixel_threshold, verbose=True)\n",
    "    x1_norm_inliers = x1_norm[:,inliers]\n",
    "    x2_norm_inliers = x2_norm[:,inliers]\n",
    "\n",
    "    P2_arr = cv.extract_P_from_E(E)\n",
    "    X_arr = cv.compute_triangulated_X_from_extracted_P2_solutions(P1, P2_arr, x1_norm_inliers, x2_norm_inliers)\n",
    "    P2, _ = cv.extract_valid_camera_and_points(P1, P2_arr, X_arr, verbose=True)\n",
    "\n",
    "    R1 = abs_rots[i]\n",
    "    T1 = abs_trans[i]\n",
    "    R2 = P2[:,:-1] @ R1\n",
    "    T2 = P2[:,-1] + (R2 @ R1.T @ T1)\n",
    "\n",
    "    abs_rots.append(R2)\n",
    "    abs_trans.append(T2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1 T_E: 10000 T_H: 10000 epsilon_E: 0.02 epsilon_H: 0.0 No. inliers: 407 From: H 4-point alg.\n",
      "Iteration: 1 T_E: 10000 T_H: 10000 epsilon_E: 0.03 epsilon_H: 0.0 No. inliers: 416 From: H 4-point alg.\n",
      "Iteration: 4 T_E: 10000 T_H: 10000 epsilon_E: 0.07 epsilon_H: 0.0 No. inliers: 1076 From: E 8-point alg.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 47 T_E: 10000 T_H: 10000 epsilon_E: 0.07 epsilon_H: 0.0 No. inliers: 1231 From: H 4-point alg.\n",
      "Iteration: 128 T_E: 10000 T_H: 10000 epsilon_E: 0.13 epsilon_H: 0.0 No. inliers: 2085 From: H 4-point alg.\n",
      "Iteration: 132 T_E: 10000 T_H: 10000 epsilon_E: 0.13 epsilon_H: 0.01 No. inliers: 2106 From: H 4-point alg.\n",
      "Iteration: 452 T_E: 10000 T_H: 10000 epsilon_E: 0.16 epsilon_H: 0.01 No. inliers: 2582 From: E 8-point alg.\n",
      "Iteration: 1245 T_E: 10000 T_H: 10000 epsilon_E: 0.2 epsilon_H: 0.01 No. inliers: 3251 From: E 8-point alg.\n",
      "Iteration: 1438 T_E: 10000 T_H: 10000 epsilon_E: 0.21 epsilon_H: 0.01 No. inliers: 3492 From: E 8-point alg.\n",
      "Iteration: 2202 T_E: 10000 T_H: 10000 epsilon_E: 0.21 epsilon_H: 0.05 No. inliers: 3551 From: H 4-point alg.\n",
      "Iteration: 2230 T_E: 10000 T_H: 10000 epsilon_E: 0.34 epsilon_H: 0.06 No. inliers: 5628 From: H 4-point alg.\n",
      "Iteration: 4499 T_E: 10000 T_H: 10000 epsilon_E: 0.36 epsilon_H: 0.06 No. inliers: 5922 From: E 8-point alg.\n",
      "Iteration: 7688 T_E: 7682.0 T_H: 10000 epsilon_E: 0.37 epsilon_H: 0.06 No. inliers: 6201 From: E 8-point alg.\n",
      "Bailout at iteration: 7688\n"
     ]
    }
   ],
   "source": [
    "# Reconstruct initial 3D points\n",
    "\n",
    "x1_init_norm = x1s_norm_TR[init_pair[1]-1]\n",
    "x2_init_norm = x2s_norm_TR[init_pair[1]-1]\n",
    "E, inliers = estimate_E_robust(K, x1_init_norm, x2_init_norm, min_its, max_its, scale_its, alpha, pixel_threshold, verbose=True)\n",
    "np.save('data/dataset_{}_TR_inliers_{}.npy'.format(data_set, init_pair[1]), inliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. valid coords for each camera pair: [12402     0  6201  6201]\n",
      "Argmax(P2_arr): 0\n"
     ]
    }
   ],
   "source": [
    "inliers = np.load('data/dataset_{}_TR_inliers_{}.npy'.format(data_set, init_pair[1]))\n",
    "x1_init_norm_inliers = x1_init_norm[:,inliers]\n",
    "x2_init_norm_inliers = x2_init_norm[:,inliers]\n",
    "\n",
    "P2_arr = cv.extract_P_from_E(E)\n",
    "X_arr = cv.compute_triangulated_X_from_extracted_P2_solutions(P1, P2_arr, x1_init_norm_inliers, x2_init_norm_inliers)\n",
    "P2, X = cv.extract_valid_camera_and_points(P1, P2_arr, X_arr, verbose=True)\n",
    "\n",
    "R1_init = abs_rots[init_pair[0]]\n",
    "X = R1_init.T @ X[:-1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cameras_and_3D_points(X, C_arr, axis_arr, s, path, save=False):\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    ax = plt.axes(projection='3d')\n",
    "\n",
    "    ax.plot(X[0], X[1], X[2], '.', ms=1, color='magenta', label='Est. X')\n",
    "    cv.plot_cameras_and_axes(ax, C_arr, axis_arr, s)\n",
    "\n",
    "    ax.set_xlabel('$x$')\n",
    "    ax.set_ylabel('$y$')\n",
    "    ax.set_zlabel('$z$')\n",
    "    ax.set_aspect('equal')\n",
    "    ax.view_init(elev=-50, azim=-104, roll=20)\n",
    "    ax.legend(loc=\"lower right\")\n",
    "    fig.tight_layout()\n",
    "    if save:\n",
    "        fig.savefig(path, dpi=300)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "P1 = np.column_stack((abs_rots[init_pair[0]], abs_trans[init_pair[0]]))\n",
    "P2 = np.column_stack((abs_rots[init_pair[1]], abs_trans[init_pair[1]]))\n",
    "P_arr = np.array([P1, P2])\n",
    "C_arr, axis_arr = cv.compute_camera_center_and_normalized_principal_axis(P_arr, multi=True)\n",
    "s = 2\n",
    "plot_cameras_and_3D_points(X, C_arr, axis_arr, s, None, save=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_T_DLT_1(img_pts, verbose=False):\n",
    "\n",
    "    n = img_pts.shape[1]\n",
    "    M = []\n",
    "\n",
    "    for i in range(n):\n",
    "\n",
    "        x = img_pts[0,i]\n",
    "        y = img_pts[1,i]\n",
    "\n",
    "        m = np.array([[1, 0, -x],\n",
    "                      [0, 1, -y]])\n",
    "        \n",
    "        M.append(m)\n",
    "\n",
    "    M = np.concatenate(M, 0)\n",
    "    U, S, VT = LA.svd(M, full_matrices=False)\n",
    "    T = VT[-1, :3]\n",
    "    print('vt last row shape', VT[-1,:].shape)\n",
    "\n",
    "    if verbose:\n",
    "        M_approx = U @ np.diag(S) @ VT\n",
    "        v = VT[-1,:]\n",
    "        Mv = M @ v\n",
    "        print('\\n||Mv||:', (Mv @ Mv)**0.5)\n",
    "        print('||v||^2:', v @ v)\n",
    "        print('max{||M - M_approx||}:', np.max(np.abs(M - M_approx)))\n",
    "        print('S:', S)\n",
    "\n",
    "    return T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_T_DLT_2(R, img_pts, verbose=False):\n",
    "\n",
    "    n = img_pts.shape[1]\n",
    "    M = []\n",
    "\n",
    "    for i in range(n):\n",
    "\n",
    "        xx = cv.create_skew_symmetric_matrix(img_pts[:,i])\n",
    "        m = np.column_stack((xx, xx @ R))\n",
    "        M.append(m)\n",
    "\n",
    "    M = np.concatenate(M, 0)\n",
    "    U, S, VT = LA.svd(M, full_matrices=False)\n",
    "    T = VT[-1, :3]\n",
    "    print('vt last row shape', VT[-1,:].shape)\n",
    "\n",
    "    if verbose:\n",
    "        M_approx = U @ np.diag(S) @ VT\n",
    "        v = VT[-1,:]\n",
    "        Mv = M @ v\n",
    "        print('\\n||Mv||:', (Mv @ Mv)**0.5)\n",
    "        print('||v||^2:', v @ v)\n",
    "        print('max{||M - M_approx||}:', np.max(np.abs(M - M_approx)))\n",
    "        print('S:', S)\n",
    "\n",
    "    return T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_T_robust(K, R, X, x_norm, min_its, max_its, scale_its, alpha, err_threshold_px, verbose=False):\n",
    "    \n",
    "    err_threshold = err_threshold_px / K[0,0]\n",
    "    best_T = None\n",
    "    best_inliers = None\n",
    "    best_epsilon = 0\n",
    "    n_points = x1_norm.shape[1]\n",
    "    n_samples = 2\n",
    "    ransac_its = max_its\n",
    "\n",
    "    t = 0\n",
    "    while t < ransac_its:\n",
    "        t += 1\n",
    "\n",
    "        rand_mask = np.random.choice(n_points, n_samples, replace=False)\n",
    "\n",
    "        print(x_norm[:,rand_mask].shape)\n",
    "        T1 = estimate_T_DLT_1(R, x_norm[:,rand_mask], verbose=False)\n",
    "        T2 = estimate_T_DLT_2(R, x_norm[:,rand_mask], verbose=False)\n",
    "        print(np.isclose(T1, T2))\n",
    "        T = T1\n",
    "        time.sleep(1)\n",
    "\n",
    "        x_norm_proj = cv.dehomogenize(R @ X + T)\n",
    "        distance_arr = compute_point_point_distance(x_norm_proj, x_norm)\n",
    "        inliers = distance_arr**2 < err_threshold**2\n",
    "        n_inliers = np.sum(inliers)\n",
    "        epsilon = n_inliers / n_points\n",
    "\n",
    "        if epsilon > best_epsilon:\n",
    "            best_T = np.copy(T)\n",
    "            best_inliers = np.copy(inliers)\n",
    "            best_epsilon = epsilon\n",
    "            ransac_its = cv.compute_ransac_iterations(alpha, best_inliers, n_samples, min_its, max_its, scale_its)\n",
    "            if verbose:\n",
    "                print('Iteration:', t, 'T:', ransac_its, 'epsilon:', np.round(best_epsilon, 2), 'No. inliers:', np.sum(inliers))\n",
    "    \n",
    "    print('Bailout at iteration:', t)\n",
    "    return best_T, best_inliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute translation registration\n",
    "\n",
    "# Is the contraint img < init_pair[0] necessary?\n",
    "# How to pair X with x? x views only some of the 3D points.\n",
    "\n",
    "min_its = 0\n",
    "max_its = 10000\n",
    "scale_its = 1\n",
    "alpha = 0.95\n",
    "P1 = cv.get_canonical_camera()\n",
    "\n",
    "for i in range(n_imgs):\n",
    "    \n",
    "    if i != init_pair[0]:\n",
    "        x1_norm = x1s_norm_TR[i]\n",
    "        x2_norm = x2s_norm_TR[i]\n",
    "        R = abs_rots[i]\n",
    "\n",
    "        T = estimate_T_robust(K, R, X, x2_norm, min_its, max_its, scale_its, alpha, 3*pixel_threshold)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_chalmers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
